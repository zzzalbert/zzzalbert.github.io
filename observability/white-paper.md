# 可观测性白皮书

原文地址：https://github.com/cncf/tag-observability/blob/main/whitepaper.md


## 目录
* [Executive Summary](#executive-summary)
* [Introduction](#introduction)
  * [Target Audience](#target-audience)
  * [Goals](#goals)
* [What is Observability?](#what-is-observability)
* [Observability Signals](#observability-signals)
  * [Metrics](#metrics)
  * [Logs](#logs)
  * [Traces](#traces)
  * [Profiles](#profiles)
  * [Dumps](#dumps)
* [Correlating Observability Signals](#correlating-observability-signals)
  * [Achieving multi-signal observability](#achieving-multi-signal-observability)
  * [Signal correlation](#signal-correlation)
  * [Practical applications](#practical-applications)
  * [Practical implementations](#practical-implementations)
* [Use cases](#use-cases)
  * [Implementing SLIs, SLOs and SLAs](#implementing-slis-slos-and-slas)
  * [Alerting on Observability data](#alerting-on-observability-data)
* [Gaps around Observability](#gaps-around-observability)
  * [Multi-signal correlation](#multi-signal-correlation)
* [Conclusion](#conclusion)
* [References](#references)
* [Contributors](#contributors)
* [Contributing](#contributing)

## 摘要

随着系统复杂性和我们每秒处理的数据的不断增长，我们需要更好的可观测性来了解我们的工作负载状态。除了可观测性工具之外，现在更常见的是期望每个负责将软件作为服务运行的工程师了解如何监控和观察他们的应用程序。随着更高的客户期望和更严格的服务水平目标，工程师必须比以往更快地调试并找到问题的根本原因。

本文旨在让你快速了解在云原生世界中工作可能需要的各种可观测性。

## 介绍

随着云计算、微服务和分布式系统的普及，新的应用程序通常被设计和构建为在云上运行。尽管这提供了构建更具弹性、更好性能和更加安全的应用程序的新策略，但它也带来了失去控制这些工作负载的基础设施的潜在风险。系统管理员、开发人员和软件操作员必须了解生产中应用程序的状态以及运行该应用程序的底层基础设施的健康状况。此外，他们应该能够从外部观察这些信号，而无需在源码中新增工具或在运行的生产代码上设置断点。

应用程序在设计和构建时应该包含和促使它们可被某些实体观测到的机制，例如，无论该实体是另一个应用程序还是无法访问数据中心的人。必须尽早做出努力，从设计开始，并且通常需要额外的代码或基础设施自动化和工具。这些文化和流程的变革通常是许多组织的挑战或障碍。最重要的是，市场上有许多方法和工具提供不同的方法来达到一个合理的可观测性水平。

ClearPath Strategies 和 Honeycomb.io 进行的社区研究[4] 表明：“四分之三的团队尚未开始或处于可观测性之旅的早期阶段”，以及“在实现更多可观测性系统的转变背后存在势头”。一旦达到令人满意的可观测性水平，其好处是毋庸置疑的，但开始会让人感觉这是一项艰巨的任务！文化变迁，不同的工具，不同的目标，不同的方法。如此多的细节需要考虑，这会让人十分混乱和痛苦。本文的目的是提供清晰的说明，以便更多的软件和运维团队能够在其系统中获得可观测性的好处。

### 目标读者

本文的目标读者是：
* 网站可靠性工程师
* 开发运维工程师
* 系统管理员
* 软件工程师
* 基础架构工程师
* 软件开发人员

本文涉及来自所有组织的上述任何角色，这些组织希望交付可观测的并与其客户现有可观测性系统集成的软件，以便达到显而易见的系统可靠性、安全性和透明度。由于可观测性是一个多学科主题，负责设计和实现软件的项目经理、产品经理、开发经理和架构师等其他利益相关者也可能对本文感兴趣。计算机科学、信息系统、工程（或相关）专业的学生和对可观测性领域感兴趣的人也可以在本文中找到有用的信息。

### 目标

云计算的使用帮助小型和大型科技公司优化成本、规模和设计更高效的产品，但它也带来了复杂性。由于基础设施现在是远程的、短暂的和全球分布的，曾经的系统管理员对数据中心的控制权现在已经不存在了。曾经在系统管理员和开发人员之间存在冲突文化的公司必须转变为一种新文化，这种新文化要求他们相互合作，旨在构建可靠的软件。在观察云原生系统的状态和帮助这些公司在这个新现实中保持系统的可靠性实践中，已经出现了一些新的策略和工具。

在可观测系统的设计和开发过程中，必须对其进行侵入以将遥测数据发送或暴露给第三方，通常是一组工具，负责从暴露的数据中提供有意义的信息。遥测数据经常以指标和日志的形式出现，软件工程团队长期使用这些数据，还有跟踪、结构化事件、性能数据和故障转储文件。每个信号都有其目的和最佳实践，它们的滥用会导致大规模运行软件时出现新问题，例如“警报疲劳”和“高成本”。
尽管存在一些新的挑战，例如文化变革、能力规划、法律问题等，但其中很多已经被早期进入这个新时代的创新公司所应对和解决。初学者可以从他们的发现和错误中吸取教训，并按照他们的最佳实践来解决同样的问题。本文将提供可观测性信号之间的区别以及应如何处理它们，列举成功公司在解决常见问题时使用的几种不同方法，介绍属于可观测性范围的几种工具以及它们应该适合你的可观测性技术栈的位置，同时抛出仍未解决的常见已知问题或在市场上仍未得到统一的方法。

## 什么是可观测性？

毫无疑问，可观测性是当今系统的理想属性。每个人都这样说，对吧？你们中的一些人可能已经开始了你的可观测性之旅，而其他人正在阅读该白皮书只是因为每个人都在说你应该让你的系统可观测。事实上，“可观测性”已经成为一个流行词，就像其他所有流行词一样，每个人都想在提倡它的同时留下自己的印记，而你所听到的可能与它的起源有着不同的含义。如果你要提高在可观测性方面的玩家级别，那就让我们一起弄清楚它的初衷吧。

“在控制理论中，可观测性是衡量一个系统的内部状态可以从其外部输出的知识中推断出来的程度”[9]。不那么理论化，它是一个系统的功能，人类和机器可以通过该功能观察、理解所述系统的状态，并采取行动。当然，根据定义，可观测性看起来很简单，但是在没有清晰目标的情况下决定系统应该有哪些输出将会变得很复杂。那时很多事情也会走偏。

刚开始时，复制别人的工作成功很容易。这是一种祝福，同时也是开源的诅咒之一。网上有很多例子；helm charts、Ansible playbook、Terraform modules，只需运行其中一个脚本，你就可以在几分钟内建立并运行可观测性技术栈。这很容易，而且对其他人也有效。因此它应该对我有用，对吧？好吧，我们并不是要鼓励你不要使用这些类型的脚本，但你必须记住，可观测性不仅仅是使用所有漂亮而耀眼的工具。你必须意识到系统输出的是什么，最最重要的是，你需要牢记一个目标！你可能会想：“哦，我想收集这个我没弄清楚的特定的数据，可能在将来会需要它”，并且你对其他数据也有这样的想法，随着数据增多你最后发现你在正在构建一个数据湖。

可观测性几乎可以用于系统开发生命周期的所有阶段。你可以在测试新功能、监控生产弹性、深入了解客户如何使用你的产品或以数据驱动考虑产品路线图时使用它。一旦你心中有了目标，你就会开始思考输出，或者我们喜欢称之为信号。

## 可观测性信号

如上所述，信号是系统产生的输出，人们或机器可以从中推断出知识。这些信号因系统而异，也取决于你想要实现的目的。它可以是你想在某个时间点测量的东西，例如温度或 RAM 使用情况，或者是你想要追踪的贯穿分布式系统的许多组件的事件。你可能想知道系统的哪个功能在随机时间点对资源（如 CPU、内存或磁盘）最密集，或者你的系统在崩溃的确切时间是如何崩溃的。虽然一些信号可能重叠以推断知识，但其他信号在系统的某些方面非常专业。它们都可以一起使用，以提供不同的方式来观察同一技术，或者，如我们建议新手一般，你可以从一个或部分其他信号开始。

你很有可能听说过“三个可观测性支柱”，即指标、日志和跟踪。它们是行业标准，可能是你要开始使用的标准。我们喜欢将它们视为“主要信号”而不是“三根支柱”，原因有二：(1) 支柱具有隐含的含义，即如果其中一根支柱缺失，整个结构就会崩溃倒塌，但这不是真的。一个人可以安全地只使用两个甚至一个信号，并且仍然实现其可观测性目标；(2) 去年，越来越多的信号在开源社区中变得流行，例如应用程序Profiles和Dumps，而今天的工具和方法仍然不能满足科技行业的所有需求。近期可能会出现新的信号，这些也是我们感兴趣并持续关注的。

![image](https://user-images.githubusercontent.com/24193764/121773601-55f86b80-cb53-11eb-8c8b-262a5aad781f.png)

所有信号都有不同的收集或检测方式。它们在获取、存储和分析时具有不同的资源成本，同时提供不同的方式来观察同一系统。在它们之间进行选择是一种权衡游戏，就像工程中的所有其他任务一样。在接下来的章节中，我们将通过深入挖掘每个信号来帮助你做出决定，首先是人们最喜欢的信号：指标、日志和跟踪，然后是两个可能的新信号：应用程序配置文件和故障转储。

___待补充包含5个信号的图___

### 指标

指标是数据的数字表示。它们分为两大类：已经是数字的数据和提炼成数字的数据。前者的典型示例是温度，后者是过程计数器。这不同于日志或跟踪，它们侧重于有关单个事件的记录或信息。

提取数据会丢失细节，例如，进程计数器无法提供有关特定增量何时发生的信息。这种权衡使指标成为最有效的信号之一：主题专家选择提炼什么以及如何提炼。这减少了保留、发送、传输、存储和处理的负载。它还可以减少人类操作员的精神负担，因为他们可以快速了解情况。

指标还表示对系统状态的时间点观察。这不同于日志或跟踪，后者侧重于有关单个事件的记录或信息。

指标通常是结构化或半结构化的，通常以两种方式使用：

* __实时监控和警报__— 指标最常见的用例是概览和深入可视化仪表板，并在受监控系统已超过阈值或行为异常时为人类或自动化系统触发警报或通知。
* __趋势和分析__ — 指标还用在基于时间维度的趋势分析和长期规划，同时还在事件发生后提供相关依据，以修复和监控潜在问题以防止再次发生。

指标提供的信息用于判断有关系统整体行为和健康状况的依据。指标通常在正在发生的“是什么”，有时是“为什么”中起着重要作用。例如，指标可以告诉你每秒 HTTP 请求的数量，但并不总能告诉你请求出现峰值或下降的原因。不过，它可以告诉你负载均衡器过载的原因。换句话说，指标并不总能揭示根本原因，通常提供定位所需的高级概览和确定问题根本原因的起点。

### 日志

日志是描述操作系统、应用程序、服务器或其他设备内的使用模式、活动和操作的文本数据流。

日志可以分为不同的类别，例如：
* __应用程序日志__ - 当应用程序内部发生事件时，会创建应用程序日志。这些日志可帮助开发人员了解和衡量应用程序在开发期间和发布后的行为方式。
* __安全日志__ - 安全日志是为响应系统上发生的安全事件而创建的。这些可能包括各种事件，例如登录失败、密码更改、身份验证请求失败、资源访问、资源更改（包括文件、设备、用户或其他管理更改）。系统管理员通常可以配置将哪些类型的事件包含在安全日志中。
* __系统日志__ - 系统日志记录操作系统本身发生的事件，例如处理物理和逻辑设备的内核级消息、引导序列、用户或应用程序身份验证以及其他活动，包括故障和状态消息。
* __审计日志__ - 也称为审计跟踪，本质上是事件和更改的记录。通常，他们通过记录执行活动的人员、执行的活动以及系统如何响应来捕获事件。通常系统管理员会根据业务需求来决定审计日志收集哪些内容。
* __基础架构日志__ - 是基础架构管理的重要组成部分，涉及管理影响组织里 IT 基础的物理和逻辑设备。这可以在本地或云端，并通过 API、Syslog 或其他使用基于主机的代理收集的方式获得。

日志可用于不同的场景——指标、跟踪、安全、调试。持续记录着所有与应用程序和系统相关的事件，让我们可以理解甚至重现导致特定情况的逐步操作。这些记录在执行根本原因分析时非常有价值，可提供信息以了解发生故障时应用程序或系统的状态。

存储在日志中的信息没有固定格式，这使得从中获取意义成为一项挑战。在过去的 30 年里，人们曾多次尝试将模式应用于日志，但都没有特别成功。使用模式的原因使得提取相关信息更容易。通常，这是通过解析、分段和分析日志文件中的文本来完成的。来自日志的数据也可以转换为其他可观测性信号，包括度量和跟踪。一旦数据成为指标，它就可以用于了解随时间的变化。日志数据也可以通过日志分析技术进行可视化和分析。

日志级别允许表达每个日志语句的重要性。一组这样的日志级别是 ERROR、WARNING、INFO 和 DEBUG。ERROR 是最不详细的级别，DEBUG 是最详细的级别。
1. __ERROR__传达故障发生的原因和详细信息。
1. __WARNING__是需要注意的高级消息，但不是故障。
1. __INFO__消息帮助我们了解系统是如何工作的。
1. __DEBUG__是存储每个操作的非常详细信息的级别。通常，仅在故障排除期间或由于存储或性能影响而短时间使用。

使用多个详细级别生成详细信息以协助进行故障排除和根因分析。

可以通过多种方式转发日志。第一个建议是配置标准流以将日志直接发送到中央位置。其次，将日志写入消息队列，以便在移动到最终目的地之前进行过滤或丰富。最后一种方法是使用开源数据收集器应用程序将日志发送到中央存储库。将日志与其他可观测性信号相结合，以全面了解你的系统。

在规划日志记录解决方案时，我们必须牢记安全性。在将日志相关文件或信息发送到中央存储库时加密静态和传输中的信息。不要在任何日志中存储任何个人身份信息 (PII)。最后，真正重要的数据不应该只保存在日志中。尽管日志语句很有用，但不能保证它们会被传送。

### 链路跟踪

分布式跟踪是一种了解分布式事务期间发生的事情的技术，例如最终用户发起的请求及其对所有下游微服务的影响。

跟踪通常是“跟踪数据点”的聚集，或者通常称为跨度，并且可以可视化为甘特图，如下例所示

![image](https://user-images.githubusercontent.com/24193764/121788584-b82d8c80-cba4-11eb-94d3-b1dd74ccf482.png)

跟踪通常代表数据传输的一个具体实例，使它们成为可观测性中的高分辨率信号。跨度是高度上下文化的，包含有关启动它的跨度的信息。这使得在分布式系统的不同参与者（如服务、队列、数据库等）之间建立因果关系成为可能。

虽然许多监控系统实施了自己专有的跟踪上下文传播方式，但业界达成了广泛的共识，即跟踪上下文传播应该以标准化方式进行。这导致了 W3C 分布式跟踪工作组的创建以及随后发布的 W3C 跟踪上下文规范。受 OpenZipkin 项目的事实标准 B3 的启发，W3C Trace Context 定义了标准的 HTTP 标头和值格式来传播启用分布式跟踪方案的上下文信息。该规范对如何在服务之间发送和修改上下文信息进行了标准化。上下文信息唯一标识分布式系统中的各个请求，还定义了添加和传播特定于提供者的上下文信息的方法。

如今，像 OpenTelemetry 这样的项目或像 .NET 这样的平台正在使用 W3C Trace Context 作为它们的标准传播格式，并且可以预期云基础设施提供商也将开始支持 W3C Trace Context，以便在通过托管服务传递时上下文不会中断，例如服务网关。

Instrumentation 在分布式跟踪中起着至关重要的作用，负责自己创建数据点并将上下文从一个服务传播到另一个服务。如果没有上下文传播，我们就无法将传入的 HTTP 请求与其下游的 HTTP 请求或消息的生产者及其消费者联系起来。

Instrumentation 有两个主要的分布式跟踪目的：上下文传播和跨度映射。在大多数情况下，上下文传播是通过使用可与 HTTP 客户端和服务器集成的库透明地完成的。在这一部分中，可以使用项目、工具和技术，例如 OpenTelemetry API/SDK、OpenTracing 和 OpenCensus 等。

![image](https://user-images.githubusercontent.com/24193764/121788568-9502dd00-cba4-11eb-9014-8fc8a9c31f05.png)
(Source: https://opentracing.io/docs/overview/)

### Profiles

随着公司继续针对云原生应用程序进行优化，以尽可能最细粒度的级别了解性能指标变得越来越重要。其他工具通常会显示存在性能问题（即延迟、内存泄漏等）。持续收集Profiles使我们能够深入了解特定系统为何会遇到此类问题。

有几种不同的分析器可用于其他用例/资源：
* CPU 分析器
* 堆分析器
* GPU 分析器
* 互斥分析器
* IO分析器
* 特定语言的分析器（例如 JVM Profiler）

在每一个中，都有许多子类型的分析，所有这些子类型都有一个相同的目标，即了解资源在系统中的分配方式。

传统上，分析被认为不适合在生产中运行，因为与系统可见性级别相关的开销很大。然而，由于采样分析器越来越受欢迎，它们在云环境中变得越来越流行；它们只增加了百分之几的开销，使生产中的分析成为一个可行的选择。

将“时间”轴添加到分析数据的能力让静态Profiles具有粒度和洞察力，使人们能够从细粒度的有利位置和鸟瞰图理解和检查他们的数据。全面了解资源对于优化/调试云原生应用程序和规划如何分配资源分配变得越来越重要。

类似于跟踪如何扩展你的选项以了解你的应用程序的哪一部分对延迟问题负责，分析可以让你更深入地研究并了解为什么存在这些延迟问题。此外，它还可以帮助你了解代码的哪些部分使用了最多的服务器资源。

运行时生成的分析数据通常包括统计信息到行号，因此它们是从代码的“什么”直接到“为什么”的重要数据。

___待插入Profile的示例图___

### Dumps

在软件开发中，核心转储文件用于对程序（即崩溃的进程）进行故障排除。传统上，操作系统在一些配置（如位置、名称约定或文件大小）的帮助下，在崩溃时写入进程内存的映像以供分析。然而，在云原生中，大型集群的核心转储文件集合很容易在存储甚至网络方面造成瓶颈，具体取决于集群的存储如何附加到集群节点。例如，处理密集型应用程序最终可能会生成两位数千兆字节大小的核心转储文件。

在基于 Linux 的系统中，可以通过全局设置 (/proc/sys/kernel/core_pattern) 将核心转储文件设置为写入系统中的任何位置。从内核 2.6+ 开始，有一种处理核心转储的新方法，即所谓的核心转储处理程序。这意味着，换句话说，不是将文件的收集委托给操作系统，而是将崩溃进程的输出推送到负责写入文件的应用程序标准输入。例如，在基于 Ubuntu 的发行版中，这可以在 systemd 或 abort 的支持下完成。基于 RedHat 的发行版使用所谓的 ABRT。

时至今日，云原生社区仍在努力收集核心转储。我们想强调至少两个主要原因：与应用程序开发人员可以访问所有旋钮来配置名称约定、大小甚至文件收集位置的系统相比，在云原生中，应用程序和基础设施所有者的角色区分不明确，因此（特权）访问全局系统设置也是更不容易。此外，云原生环境固有的一个方面是数据持久性：崩溃的应用程序（例如 pod）在收集其核心转储文件以在重启前写入持久卷时需要帮助。

大约 5 年的 RFC ( https://lore.kernel.org/patchwork/patch/643798/ ) 请求 Linux 内核社区中的命名空间 core_pattern 支持，而不是将其作为全局系统设置。此外，Docker 社区有一个关于同年开放的问题 ( moby/moby#19289 )，要求在 Docker 中提供 core_pattern 支持。

## 关联可观测性信号

毫无疑问，可观测性是复杂的。正如你从前面的章节中了解到的，为了更多地了解我们运行的软件的状态和行为，我们从不同的角度、不同的时间间隔和管道收集不同的数据类型：

* 指标：一段时间内状态的可聚合数字表示。
* 日志：表示离散事件的结构化或/和人类可读的详细信息。
* 跟踪：可以绑定到系统中单个实体（例如请求或事务）的生命周期的元数据位。

我们还讨论了不属于上述类别的数据，它们开始获得自己的“信号”徽章，例如：

* 继续分析：随着时间的推移，不同程序功能中各种资源的代码级消耗量（例如，使用的内存、花费的 CPU 时间）。

我们想到的第一个问题是，为什么我们要创建这么多类型？我们不能只有一个，“一网打尽”吗？问题是我们不能，以同样的方式，我们不能拥有一辆既能在柏油马路上又能在越野路上高效工作的自行车。每种类型的信号都因其用途而高度专业化。__指标__以实时、可靠和廉价的监控为中心，支持第一响应警报 - 可靠系统的基础。我们收集__日志__，让我们更深入地了解有关正在运行的系统的更小细节，以获得更多上下文。在某个时候，详细信息形成了一个请求树，因此__分布式跟踪__开始发挥其跨度和跨进程上下文传播的作用。有时我们需要更深入地研究，我们跳入__性能应用程序Profiles__以检查哪段代码效率低下并使用了意想不到的资源量。

正如你可能已经注意到的那样，只有一个信号不足以提供一个完整、方便的可观测性故事。例如，将太多详细信息放入__指标__（基数）中成本太高，并且以警报所需的近实时延迟可靠地__跟踪__每个可能的操作成本太高。这就是为什么我们看到许多组织旨在为他们的可观测性故事安装和利用多个信号。

### 实现多信号可观测性

多信号可观测性是可行的，许多人已经实现了。不过，当你退一步看看要实现这一目标必须构建哪些东西时，你会发现一些主要的挑战、错失的机会或效率低下的问题：

1. 不同操作的工作量

除非你愿意花钱买一个 SaaS 解决方案，它会为你做一些工作，否则现在很难让一个团队管理所有的可观测性系统。有一个单独的专门团队来安装、管理和维护每个可观测性信号，例如一个用于度量系统、一个用于记录堆栈、一个用于跟踪。这是由于每个系统需要不同的设计模式、技术、存储系统和安装方法。这里的碎片是巨大的。这就是我们旨在通过开源计划来改进的目标，例如用于检测和转发部件的[OpenTelemetry](https://opentelemetry.io/) 以及用于可扩展多信号后端的[Obsevatorium](https://observatorium.io/)。

2. 重复的工作量

![image](https://user-images.githubusercontent.com/24193764/121791131-ecad4280-cbbc-11eb-9542-0b940f6a5846.png)

当我们查看每个提到的可观测性信号的有效负载信息时，会发现有明显的重叠。例如，让我们看一下关于上图中可见目标的数据的即时收集。我们看到关于“数据在哪里”的上下文（通常称为“目标元数据”）对于每个信号都是相同的。然而，因为每个信号背后都有一个独立的系统，我们往往会多次发现这些信息，而且往往不一致，将这些信息保存在多个地方，并且（更糟糕的是！）多次索引和查询它。

而且它不仅适用于目标元数据。许多事件会产生多个信号：增量指标、触发日志和跟踪范围。这意味着与此特定事件相关的元数据和上下文在整个系统中都是重复的。在开源项目中，人们慢慢尝试减轻这种影响，例如[Tempo](https://github.com/grafana/tempo)项目。

3. 不同信号间的提取集成

考虑到多信号管道，通常需要用来自另一个信号的额外数据来补充每个系统。比如将兼容典型指标度量协议（例如 OpenMetrics/Prometheus）的特定跟踪和日志行集合转换成指标度量，或者类似地将日志行组合到提取的跟踪路径上的功能。[OpenTelemetry收集处理器](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/spanmetricsprocessor)从跟踪跨度生成 RED 指标和[Loki](https://grafana.com/blog/2020/10/28/loki-2.0-released-transform-logs-as-youre-querying-them-and-set-up-alerts-within-loki/)将日志转换为指标的功能等是该领域的一些现有改进。

4. 信号使用时的集成

同样，在“查看”级别上，快速导航到表示相同或相关事件的另一个可观测性信号将非常有用。这就是我们所说的信号相关性。让我们详细关注这个机会。现在可以实现什么？

### 信号相关

为了将可观测性数据链接在一起，让我们看一下附加到所有信号的常见（如前所述，有时是重复的）数据

![image](https://user-images.githubusercontent.com/24193764/121791172-62191300-cbbd-11eb-93a5-6262d87c7873.png)

由于收集所有可观测性信号的连续形式，每条数据都在某个时间戳范围内。这使我们能够在__特定时间窗口__（有时精确到毫秒）内过滤信号数据。在不同的维度上，如上图呈现的场景，每一个可观测性信号通常都与某个“目标”绑定。要识别目标，必须存在__目标元数据__，这在理论上允许我们查看来自特定目标的指标、性能数据、跟踪和日志行。为了进一步缩小范围，将额外的元数据添加到收集可观测性数据的__代码组件__的所有信号中并不少见，例如“工厂”。

![image](https://user-images.githubusercontent.com/24193764/121791201-b15f4380-cbbd-11eb-9dd6-be55400d08a6.png)

这本身就非常强大，因为它允许我们通过选择每个信号与特定过程或代码组件和时间的关联条目，从每个信号中快速导航。考虑到这一点，一些前端（如 Grafana）已经允许创建此类链接和侧视图。

但这还没有结束。我们有时会有更多详细信息，有时会附加到跟踪和日志记录中。特别是，分布式跟踪通过将所有跨度限制在单个__trace ID__下来获得它的能力。为了同一用户请求，此信息从一个函数传播到另一个函数，从一个进程传播到另一个进程，再到链接操作。在你的日志行中共享与请求相关的相同信息并不少见，有时称为__Request ID__或__Operation ID__。通过确保日志记录和跟踪之间的那些 ID 完全相同的简单方法，我们在如此低级别的范围内使他们相互关联。这使我们能够轻松地在绑定到单个请求的日志行、跟踪跨度和标签之间导航。

![image](https://user-images.githubusercontent.com/24193764/121791219-e2d80f00-cbbd-11eb-8696-09dfd226aff1.png)

虽然这样的关联级别对于某些用例来说可能已经足够好了，但我们可能会遗漏一个重要的关联：大规模！如此大的系统中的进程不会处理几个请求。它们出于截然不同的目的和效果执行数万亿次操作。即使我们可以从单个进程中获取所有日志行或跟踪，哪怕是一秒钟，你如何从当时正在处理的数千个并发请求中找到与你的目标相关的请求、操作或跟踪ID？ 强大的日志记录语言（例如[LogQL](https://grafana.com/docs/loki/latest/logql/)）允许你通过查询匹配日志行获取日志级别、错误状态、消息、代码文件等详细信息。但是，这需要你了解可用字段、它们的格式以及它如何映射到应用程序中的某个场景。

如果针对大量特定错误或某些端点的高延迟的警报让你知道所有受影响的请求ID，那不是更好吗？此类警报可能基于__指标__，并且此类指标在某些请求流期间存在增量，这很可能还产生了__日志行或跟踪__并为其分配了__请求、操作或跟踪ID__，对吗？

这听起来不错，但正如我们所知，此类聚合数据（如指标或一些结合了多个请求结果的日志行）是设计聚合的（惊喜！）。出于成本和焦点原因，我们无法传递作为聚合一部分的所有（有时是数千个）请求ID。但是关于我们可以利用的这些请求有一个有用的事实。在这样一个聚合指标或日志行的上下文中，所有相关请求都是……有些相等！因此可能不需要保留所有 ID。我们可以只附上一个，代表一个案例。这就是我们所说的__范例__。

> [Exemplar](https://dictionary.cambridge.org/dictionary/english/exemplar): 其他某方面的一个典型案例.

![image](https://user-images.githubusercontent.com/24193764/121791244-2a5e9b00-cbbe-11eb-82f5-aa4d87faa3f3.png)

我们可以在一个完美的可观测性系统中使用混合的所有关联信息，这使我们能够灵活地从多个信号/视角检查我们的系统。

理论上，我们也可以将示例附加到性能数据，但考虑到它的专业化和用例（进程内性能调试），实际上我们很少需要关联到单个请求跟踪或日志行。

### 实际应用

我们讨论了在信号之间导航的方法，但它真的有用吗？让我们非常简要地看看两个基本示例：

![image](https://user-images.githubusercontent.com/24193764/121791411-03a16400-cbc0-11eb-8183-e8124cf0947f.png)

* 我们收到了关于超出我们 SLO 的异常高错误率的警报。警报基于错误计数器，我们看到请求激增导致 501 错误。我们以__范例__定位到示例日志行，以了解确切的人性化错误消息。由此发现错误来多个跃点后面的一个内部微服务，因此得益于存在与__跟踪ID__匹配的__请求ID__ ，我们定位到链路信息。多亏了这一点，我们确切地知道是什么服务/进程导致了这个问题，并在那里进行了更多挖掘。

![image](https://user-images.githubusercontent.com/24193764/121791428-1c117e80-cbc0-11eb-9f12-39a2de1366f1.png)

* 我们调试分析慢请求时，我们通过跟踪采样手动触发请求并获取__跟踪ID__。借助tracing视图，我们可以看到在请求途中的几个过程，是一个ABC-1请求，对于基本操作来说慢得惊人。由于目标元数据和时间，我们选择了相关的 CPU 使用率指标。我们看到 CPU 使用率很高，接近机器限制，表明 CPU 饱和。要了解为什么 CPU 使用如此频繁（尤其是当它是容器中的唯一进程时），我们使用相同的__目标元数据__和__时间__选择导航到 CPU 性能数据。

### 实际实施

在实践中是否可以实现？是的，但根据我们的经验，知道如何构建它的人并不多。争夺这个空间的碎片化和大量不同的供应商和项目可能会混淆概述并隐藏一些简单的解决方案。幸运的是，开源社区为简化和商品化这些方法做出了巨大努力。让我们看看实现这种平滑的多信号相关设置的一些开源方法。为简单起见，假设你已经选择并定义了一些指标、日志记录和跟踪堆栈（在实践中，通常跳过日志记录或跟踪以提高成本效率）。

从高层来看，我们需要确保三个要素：

1. 一致的__目标__元数据附加到所有信号

这可能已经是一项艰巨的任务，但我们可以走一些捷径。这种快捷方式称为__pull模型__。例如，在 Prometheus 系统中，一致的元数据要容易得多，这要归功于针对目标指标收集的单一、集中管理的发现服务。在许多其他好处中，pull模型允许度量客户端（例如你的 Go 或 Python 应用程序）只关心它自己的度量元数据，完全忽略它运行的环境。相反，这对于push模型系统来说很难维护，这种系统跨越了主流日志记录和跟踪收集管道（例如 Logstash、非拉式 OpenTelemetry 接收器、Fluentd 的非拖尾插件、Fluentbit）。想象一下，一个应用程序定义了它在运行中的节点为`node`，而另一个在标签中提到它为`machine`，还有一个将其放入`instance`标签中。

在实践中，我们有几种选择：

* 假设我们坚持使用push模型（对于某些情况，例如强制执行批处理作业）。在这种情况下，我们需要确保我们的客户端跟踪、日志记录和指标实现添加正确且一致的目标元数据。跨编程语言的标准代码库会有所帮助，尽管我们使用的所有第三方软件（想想，例如 Postgres）在实践中采用这些代码库需要时间（数年！）。然而，如果你控制你的软件，这并非不可能。服务网格可能对标准进入/退出可观测性有所帮助，但会禁用任何开箱可观测性。实现此目的的另一种方法是使用处理插件，例如 OpenTelemetry，提供动态重写元数据（有时称为重新标记）。不幸的是，它在实践中可能很脆弱，并且随着时间的推移难以维护。
* 第二个选项是使用并更喜欢pull模型，并在管理员/操作员角度定义目标元数据。我们已经在[Prometheus](https://prometheus.io/)或[Agent]((https://github.com/grafana/agent)的开源中做到了这一点，这要归功于[OpenMetrics](https://openmetrics.io/)不断抓取指标和[Parca](https://github.com/parca-dev/parca)对性能数据做同样的事情。同样，已经有很多解决方案可以从标准输出/错误中拖尾日志，例如[Promtail](https://grafana.com/docs/loki/latest/clients/promtail/)或[OpenTelemetry tailing](https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/logs/overview.md#via-file-or-stdout-logs)收集器。不幸的是，我们还不知道有任何实现可以提供来自某种媒介的拖尾链路跟踪（目前）。

2. 使同一个事情的操作ID、请求ID 或跟踪ID 附加到日志系统。

这部分必须在仪表级别上得到保证。多亏了 OpenTelemetry 上下文传播和 API，我们可以通过获取跟踪ID（理想情况下，仅当跟踪被采样）并将其添加到与此类请求相关的所有日志行中，在我们的代码中非常轻松地完成此操作。使其统一的一个非常好的方法是利用中间件 (HTTP) 和[拦截器 (gRPC](https://github.com/grpc-ecosystem/go-grpc-middleware)编码范例。值得注意的是，即使你不想使用跟踪系统或者你的跟踪采样非常严格，在你的日志记录中生成和传播请求ID仍然很有用。这允许将单个请求的日志行关联在一起。

3. 典范

范例在开放源代码领域有些新鲜，所以让我们看一下目前可行的方法以及如何采用它们。将范例添加到你的日志系统非常简单。我们可以为聚合多个请求的日志行添加一个简单键值标签形式的范例`exemplar-request=<traceID>`。

为公制添加范例是另一回事。这可能值得我某天写一篇单独的文章，但正如你想象的那样，我们通常不能将请求或跟踪ID 直接添加到指标序列的元数据（例如 Prometheus 标签）。这是因为它会创建另一个只有一个样本的一次性使用的独特序列（导致无限的“基数”）。然而，在开源中，最近，我们可以使一种非常新颖的模式，由[OpenMetrics, 名为Exemplar](https://github.com/OpenObservability/OpenMetrics/blob/main/specification/OpenMetrics.md#exemplars)定义的。这是附加信息，附加到（任何）系列样本，在主要（高索引）标签之外。这是它在 OpenMetrics 文本格式中的样子，例如 Prometheus：

```
# TYPE foo histogram
foo_bucket{le="0.01"} 0
foo_bucket{le="0.1"} 8 # {} 0.054
foo_bucket{le="1"} 11 # {trace_id="KOO5S4vxi0o"} 0.67
foo_bucket{le="10"} 17 # {trace_id="oHg5SJYRHA0"} 9.8 1520879607.789
foo_bucket{le="+Inf"} 17
foo_count 17
foo_sum 324789.3
foo_created  1520430000.123
```

一旦定义，它们就会与 OpenMetrics 兼容的抓取器（例如 Prometheus）一起抓取度量样本（确保在你的检测客户端中启用 OpenMetrics 格式）。完成后，你可以方便地通过[Prometheus 社区定义的__Exemplars API__](https://prometheus.io/docs/prometheus/latest/querying/api/#querying-exemplars)查询这些范例：

```
curl -g 'http://localhost:9090/api/v1/query_exemplars?query=test_exemplar_metric_total&start=2020-09-14T15:22:25.479Z&end=020-09-14T15:23:25.479Z'
{
    "status": "success",
    "data": [
        {
            "seriesLabels": {
                "__name__": "test_exemplar_metric_total",
                "instance": "localhost:8090",
                "job": "prometheus",
                "service": "bar"
            },
            "exemplars": [
                {
                    "labels": {
                        "traceID": "EpTxMJ40fUus7aGY"
                    },
                    "value": "6",
                    "timestamp": 1600096945.479,
                }
            ]
        },
       (...)
```

请注意，该`query`参数不适用于某些神奇的 ExemplarsQL 语言或其他语言。此 API 需要你可能在仪表板、警报或规则上使用过的任何 PromQL 查询。该实现应该解析查询所有使用的序列，如果存在，将返回这些范例相关的所有序列。

这个 API 很快被 Grafana 采用，即使是现在，你也可以在最新版本的 AGPLv3 许可的 Grafana 上呈现范例并允许快速链接到跟踪视图。

当然，这只是基础。Prometheus在2021年初完成了完整的基础设施和逻辑，以支持样本的抓取、存储、查询，甚至在远程写入中复制这些样本。[Thanos](http://thanos.io/)开始支持范例，因此 Grafana也支持了。

还值得一提的是，OpenTelemetry 还从 OpenCensus 继承了某种形式的范例。这些与 OpenMetrics 非常相似，只是只能附加到直方图桶上。然而，我们不知道任何人在任何地方使用或依赖 Otel 度量协议的这一部分，包括[Go]((https://github.com/open-telemetry/opentelemetry-go/issues/559)等相关实现。这意味着，如果你想要一个稳定的相关性，以及一个已经在运行的生态系统，OpenMetrics 可能是前进的方向。另外，[OpenTelemetry 也在慢慢采用 OpenMetrics](https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/metrics/semantic_conventions/openmetrics-guidelines.md)。

## 用例

### 基于盒子的监控类别

监控可以分为两类：

* 白盒监控
* 黑盒监控

一般来说，黑盒监控是指从外部观察系统，操作员无法控制或了解系统的内部运作。另一方面，白盒监控指的是更“传统”的概念，即监控你控制的应用程序或系统并了解其功能，以便你能够更好地决定如何通过以下方式对其进行观察可观测性的三大支柱。

### 实现 SLI、SLO 和 SLA

实施 SLI、SLO 和 SLA 指标可以让你客观地衡量服务质量和客户满意度。更重要的是，它在组织内的不同职能（如业务、产品和工程）之间提供了一组通用术语。工程时间在任何组织中都是稀缺资源，但每个人都觉得他们的问题是一个紧迫的问题。SLO 使此类对话更加受数据驱动，因为每个人都了解违反 SLO 的业务后果。在解决内部冲突的同时，它还通过提供有意义的抽象来启用有意义且可操作的警报，从而使你更加关注客户。

在我们深入研究实现细节之前，我们应该弄清楚定义，因为它们可能相当混乱，有时可以互换使用。

* 服务水平指标 (SLI)：SLI 是一种服务水平指标，是对所提供服务水平的某些方面精心定义的量化指标。
* 服务级别目标 (SLO)：SLO 是服务级别目标：目标是你可以承受失败的频率。由 SLI 测量的服务级别的目标值或值范围
* 服务级别协议 (SLA)：包含违反 SLO 的后果的业务合同。这是一个目标百分比的错误预算：在 SLO 确定的一段时间内对失败事件的容忍度。这是 100% 减去 SLO

为了使拟议的 SLO 有用且有效，你需要让所有利益相关者都同意它。产品经理必须同意这个阈值对用户来说已经足够好了 —— 低于这个值的性能低得令人无法接受，值得花费工程时间来修复。产品开发人员需要同意，如果错误预算已经用尽，他们将采取一些措施来降低用户面临的风险，直到服务回到预算之内。负责捍卫此 SLO 的生产环境团队一致认为，不付出巨大的努力、过度的辛劳和倦怠 —— 所有这些都会损害团队和服务的长期健康。

![sli:o:a](https://user-images.githubusercontent.com/8470415/124545037-ede13080-de45-11eb-860f-d314625aebed.png)

### 对可观测性数据发出警报

在广泛采用指标收集之前，大多数软件系统仅依靠日志来解决和分类问题并获得对其系统的可见性。除了日志搜索和仪表板之外，日志还充当许多团队和工具的主要警报源。这种方法今天仍然存在于许多现代可观测性系统中，但通常应该避免使用，以支持对时间序列指标发出警报。更具体地说，我们将研究使用你定义的 SLO 和错误预算来执行可操作的警报。

在你的时间序列数据中有许多你可以发出警报的信号，其中许多可能是特定于应用程序的。推荐的最佳做法是使用你团队的 SLO 来驱动你的警报。如上所述，SLO 是服务水平目标，即服务水平的目标值或值范围，由服务水平指标衡量。例如，REST API 的 SLO 可能是“95% 的请求必须在不到 500 毫秒内得到满足”。为了为你的团队提供有效的警报，你还应该定义错误预算。我们将研究如何结合你的 SLO 和错误预算来推动可操作的警报。

#### __报警实践__

构建警报可能非常复杂，很容易被误报淹没并产生警报疲劳。警报应该是可操作的，并指示某人需要采取行动的问题。我们将在下面查看你可以实施的两种方法，一种是简单的方法，另一种是基于燃烧率的方法。

##### __目标错误率__

对目标错误率发出警报是你可以采用的最简单的方法。选择一个小时间窗口，比如 10 分钟，如果该窗口中的错误率超过你的 SLO，则发出警报。

例如，如果你的 SLO 为 99.9%，则在过去 10 分钟的错误率 >= 0.1% 时发出警报。在 Prometheus 中，这可能看起来像这样（总 HTTP 请求错误除以过去 10 分钟内所有请求的总和）：
```
(sum(rate(http_requests_total{code=~"5.."}[10m])) / sum(rate(http_requests_total[10m]))) > 0.001
```
这样做的好处是可以简单明了地查看警报逻辑中发生的情况，并在遇到错误时快速发送警报。但是，此警报可能会在许多不违反你定义的 SLO 的事件上触发。

##### __燃烧率__

燃烧率警报是一种更复杂的方法，可能会产生更多可操作的警报。首先，让我们更详细地定义什么是消耗率和错误预算。

所有 SLO 定义中固有的是错误预算的概念。通过声明 99.9% 的 SLO，你设定 0.1% 的故障率（即你的错误预算）在某个预定义的时间量（你的 SLO 窗口）内是可以接受的。“Burn rate 是指相对于 SLO，服务消耗错误预算的速度有多快”[8]。因此，例如，如果“服务使用 1 的消耗率，这意味着它消耗错误预算的速率使你在 SLO 时间窗口结束时的预算正好为 0。一段时间内 SLO 为 99.9% 30 天的窗口，恒定的 0.1% 错误率恰好使用了所有错误预算：燃烧率为 1。” [8]

![image](https://user-images.githubusercontent.com/24193764/121790715-74448280-cbb8-11eb-9b66-ea432377449f.png)
(Errors relative to burn rate[8])


| Burn rate | Error rate for a 99.9% SLO | time to exhaustion |
|-----------|----------------------------|--------------------|
| 1         | 0.1%                       | 30 days            |
| 2         | 0.2%                       | 15 days            |
| 10        | 1%                         | 3 days             |
| 1000      | 100%                       | 43 minutes         |
(Burn rates and time to complete budget exhaustion[8])

燃烧率将使我们能够减小窗口的大小并创建具有良好检测时间和高精度的警报。对于我们的示例，假设将警报窗口固定在一个小时，并确定 5% 的错误预算支出足以通知某人，你可以得出用于警报的消耗率。

对于基于消耗率的警报，警报触发所需的时间为：
```
(1 - SLO / error ratio) * alerting windows size * burn rate
```
警报触发时消耗的错误预算是：
```
(burn rate * alerting window size) / time period
```
因此，花费超过一小时的 30 天错误预算的百分之五需要消耗率为 36。警报规则现在变为：
```
(sum(rate(http_requests_total{code=~"5.."}[1h])) / sum(rate(http_requests_total[1h]))) > 36 * 0.001
```

## 可观测性的难点

### 多信号关联

希望上面的文章很好地解释了如何考虑可观测性相关性、它意味着什么以及现在可以实现什么。然而，让我们快速列举当今多信号可观测性链接的缺陷：

* 元数据不一致

如前所述，即使标签之间存在轻微的不一致，在使用时也可能会很烦人。重新标记技术或默认使用拉模型会有所帮助。

* 缺少请求 ID 或与记录信号中的跟踪 ID 不同的 ID

如前所述，这可以在仪器方面解决，这有时很难控制。中间件和服务网格也可以提供帮助。

* 棘手的跟踪抽样案例

收集所有请求的所有跟踪和跨度可能非常困难。这就是为什么该项目定义了不同的采样技术，只允许“采样”（因此收集）那些以后可能有用的记录。明白哪些重要是很必要的，因此出现了复杂的抽样。主要问题是确保日志系统中的示例或直接跟踪 ID 等相关点指向采样跟踪。如果我们的前端系统会公开不可达的链接示例（存储中没有可用的跟踪），那将是糟糕的用户体验。

虽然可以在 UI 方面改进这种体验（例如，在呈现示例之前预先检查跟踪是否存在），但它并非微不足道，并且会进一步增加系统的复杂性。理想情况下，我们可以在将示例注入到公制系统中之前检查是否对跟踪进行了采样。如果使用前期采样方法，OpenTelemetry 编码 API 允许通过方法获取采样信息IsSampled。如果我们谈论基于尾部的采样或可能分析哪些跟踪是有趣的或不感兴趣的进一步过程，问题就会出现。我们还没有看到一些更好的想法来改善这个小而烦人的问题。如果你有 100% 抽样或前期抽样决策（请求比率或用户选择），此问题就会消失。

* 示例在生态系统中是新的。

Prometheus 用户体验特别好，是因为在你的应用程序中使用 Prometheus/OpenMetrics 标准。世界各地的软件都使用这种简单的机制来添加大量有用的指标。因为 Prometheus 范例是新的，OpenTelemetry 跟踪库也是新的，人们需要时间来开始使用范例“检测他们的仪器”。

但！你可以从你自己的案例开始，向你的应用程序添加 Prometheus 范例支持。这种关联模式正在成为一个新标准（例如在 Thanos 中进行检测），因此通过添加它们来帮助你自己和你的用户，并允许在跟踪和指标之间轻松链接。

* 更高级别的度量聚合、下采样。

尚待添加的是添加用于记录规则和警报的范例的能力，这些范例可能会聚合附加范例的更多指标。这已被提出，但工作尚未完成。同样，我们为 Prometheus 的进一步迭代讨论的下采样技术必须考虑下采样示例。

* UI 中的原生关联支持

Grafana 在多信号链接方面处于领先地位，但鉴于本文分享的方式，许多其他 UI 会使用更好的关联支持。在 Grafana 之前，空间非常分散（每个信号通常都有自己的视图，很少考虑其他信号）。Prometheus UI 也不例外，也计划添加关联其他信号或渲染示例的[额外支持](https://github.com/prometheus/prometheus/issues/8797)。

## 参考

<!-- TODO: please add extra references here -->
1. HARTMANN, Richard. Talk given at Fosdem (Brussels), Feb 2019. Available at: https://archive.fosdem.org/2019/schedule/event/on_observability_2019/. Accessed on: June 24, 2021.
1. SRIDHARAN, Cindy. _Distributed Systems Observability_. **Chapter 04, The Three Pillars of Observability**. 2018. Available at: https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/ch04.html. Accessed on: June 24, 2021.
1. BEYER, Betsy; JONES, Chris; MURPHY, Niall; PETOFF, Jennifer. _Site Reliability Engineering_. O'Reilly Media, 2016. Available at: https://sre.google/sre-book/table-of-contents/. Accessed on: June 24, 2021.
1. BEYER, Betsy; MURPHY, Niall; RENSIN, David; KAWAHARA, Kent; THORNE, Stephen. _The Site Reliability Workbook_. O'Reilly Media, 2018. Available at: https://sre.google/workbook/table-of-contents/. Accessed on: June 24, 2021.
1. SRIDHARAN, Cindy. _Monitoring and Observability_. Sep 5, 2017. Available at: https://copyconstruct.medium.com/monitoring-and-observability-8417d1952e1c. Accessed on: June 24, 2011.
1. MCCARTHY, Kate; FONG-JONES, Liz; FISHER, Danyel; MAHON, Deirdre; PERKINS, Rachel. _Observability Maturity: Community Research Findings Q1, 2020_. April, 2020. Available at: https://www.honeycomb.io/wp-content/uploads/2020/04/observability-maturity-report-4-3-2020-1-1.pdf. Accessed on: June 24, 2021.
1. Kalman R. E., _On the General Theory of Control Systems_, Proc. 1st Int. Cong. of IFAC, Moscow 1960 1481, Butterworth, London 1961. Available at: https://www.sciencedirect.com/science/article/pii/S1474667017700948?via%3Dihub. Accessed on: June 24, 2021.

## 贡献者

从第一句话写到最终完成，这份白皮书是社区的努力。在我们双周会议的同步讨论中, 在对我们草稿文档 [#tag-observability slack-channel](https://cloud-native.slack.com/archives/CTHCQKK7U) 的评论和意见的异步讨论中，我们的贡献者比我们预期的要多得多。以下是在这几个月帮助过我们的贡献者，名字按字母顺序。

* [Alex Jones][Alex Jones]
* [Arthur Silva Sens][Arthur Silva Sens]
* [Bartłomiej Płotka][Bartłomiej Płotka]
* [Charles Pretzer][Charles Pretzer]
* [Daniel Khan][Daniel Khan]
* [David Grizzanti][David Grizzanti]
* [Debashish Ghatak][Debashish Ghatak]
* [Dominic Finn][Dominic Finn]
* [Frederic Branczyk][Frederic Branczyk]
* [Gibbs Cullen][Gibbs Cullen]
* [Jason Morgan][Jason Morgan]
* [Jonah Kowall][Jonah Kowall]
* [Juraci Paixão Kröhling][Juraci Paixão Kröhling]
* [Ken Finnigan][Ken Finnigan]
* [Krisztian Fekete][Krisztian Fekete]
* [Liz Fong-Jones][Liz Fong-Jones]
* [Matt Young][Matt Young]
* [Michael Hausenblas][Michael Hausenblas]
* [Rafael Natali][Rafael Natali]
* [Richard Anton][Richard Anton]
* [RichiH Hartmann][RichiH Hartmann]
* [Rob Skillington][Rob Skillington]
* [Ryan Perry][Ryan Perry]
* [Shelby Spees][Shelby Spees]
* [Shobhit Srivastava][Shobhit Srivastava]
* [Simone Ferlin][Simone Ferlin]
* [Tim Tischler][Tim Tischler]
* [Wiard van Rjj][Wiard van Rjj]

感谢大家!

<!-- Please add other contributors here -->
[Alex Jones]:             https://github.com/AlexsJones
[Arthur Silva Sens]:      https://github.com/ArthurSens
[Bartłomiej Płotka]:      https://github.com/bwplotka
[Charles Pretzer]:        https://github.com/cpretzer
[Daniel Khan]:            https://github.com/danielkhan
[David Grizzanti]:        https://github.com/dgrizzanti
[Debashish Ghatak]:       https://github.com/wallydrag
[Dominic Finn]:           https://github.com/dofinn
[Frederic Branczyk]:      https://github.com/brancz
[Gibbs Cullen]:           https://github.com/gibbscullen
[Jason Morgan]:           https://github.com/wmorgan
[Jonah Kowall]:           https://github.com/jkowall
[Juraci Paixão Kröhling]: https://github.com/jpkrohling
[Ken Finnigan]:           https://github.com/kenfinnigan
[Krisztian Fekete]:       @
[Liz Fong-Jones]:         https://github.com/lizthegrey
[Matt Young]:             https://github.com/halcyondude
[Michael Hausenblas]:     https://github.com/mhausenblas
[Rafael Natali]:          https://github.com/rafaelmnatali
[Richard Anton]:          @
[RichiH Hartmann]:        https://github.com/RichiH
[Rob Skillington]:        https://github.com/robskillington
[Ryan Perry]:             https://github.com/Rperry2174
[Shelby Spees]:           https://github.com/shelbyspees
[Shobhit Srivastava]:     https://github.com/SinisterLight
[Simone Ferlin]:          https://github.com/sferlin
[Tim Tischler]:           @
[Wiard van Rjj]:          https://github.com/wiardvanrij


## 贡献

这份白皮书还未完成！如果你想帮助我们，这里还有一些希望包含进来的主题：

* *提高你的可观测性 — “投资回报”* - 实现细粒度的可观测性可能非常昂贵，尤其是如果很天真地去做。通常，我们建议从简单开始，然后使用更多信号、更多粒度和数据进行迭代。让我们在一个新的章节中讨论这些建议。我们还列举了我们应该如何迭代以及必须满足哪些标准才能继续进行可观测性扩展。
* 用例
  * *数据可视化和探索* - 构建仪表板非常简单，但构建**好的**仪表板是另一回事。我们想讲述仪表板构建的最佳实践，并解释构建它们的方式如何根据我们想要实现的目标而有所不同。例如：分析历史数据，实时监控。
* 可观测性周边的问题
  * *机器学习，异常检测和分析* — 关于 ML/Analytics 在可观测性领域是否有用，有一些非常强烈和反对的意见。我们需要有经验的人向社区解释在哪些地方为什么有用，在哪些地方为什么不能用。
  * *监控流式的API* - 今天有两种非常著名的监控方法。USE 方法监控计算资源，RED 方法监控基于请求的服务。这两种方法都不适用于流式API，而且随着远程过程调用 (RPC) 的普及，我们需要想出一个清晰的方法来监控它们。
  * *eBPF* - eBPF 是一项革命性的技术，可以在 Linux 内核中运行沙盒程序，而无需更改内核源码或加载内核模块。凭借一些创造力，它可以帮助我们实现在不添加额外工具的情况下观察系统的长期梦想。尽管有无限的可能性，但 eBPF 并不是那么流行。我们想告诉我们的读者， eBPF 和相关工具如何成为可观测性领域的游戏规则改变者，以及为什么它们仍然不那么流行。
  * *观察短周期系统* — FaaS/Serverless 领域的可观测性仍然很弱，我们想告诉我们的读者为什么观察这种系统如此困难。基于 Pull 和 Push 的模型如何与它们交互，以及为什么性能开销长期以来一直是一个问题。
